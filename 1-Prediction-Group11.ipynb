{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Packages"
   ],
   "id": "e37c547eb498e4bf"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error\n",
    "# to install below skopt package, write in terminal:\n",
    "# pip install scikit-optimize\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "import pickle"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Import"
   ],
   "id": "fc557d443bb0ebbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"DataTrain-2.csv\")"
   ],
   "id": "2592afc51de76bfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Check for null values and infinite values"
   ],
   "id": "1051f485fd9dc5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Checking which columns have any null values\n",
    "columns_with_nulls = df.isnull().any()\n",
    "\n",
    "# Filtering to show only columns with null values\n",
    "columns_with_nulls_true = columns_with_nulls[columns_with_nulls].index.tolist()\n",
    "\n",
    "print(\"Columns with null values:\", columns_with_nulls_true)\n",
    "\n",
    "# Check for columns with infinite values (positive or negative)\n",
    "infinite_columns = df.isin([np.inf, -np.inf]).any()\n",
    "\n",
    "# Filter and print column names that have infinite values\n",
    "columns_with_infinity = infinite_columns[infinite_columns].index.tolist()\n",
    "print(\"Columns with infinite values:\", columns_with_infinity)"
   ],
   "id": "fefd22025dc31ad7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Create a new column RUL "
   ],
   "id": "6ad8eead5fdd7344"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating a new column with the max 'cycle' per 'engine_id'\n",
    "df['max_cycle_per_engine'] = df.groupby('engine_id')['cycle'].transform('max')\n",
    "# Creating the 'cycles_left' column\n",
    "df['RUL'] = df['max_cycle_per_engine'] - df['cycle']\n",
    "df.drop(['max_cycle_per_engine'], axis=1, inplace=True)"
   ],
   "id": "8874f1b62c85add8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Remove features with zero variance"
   ],
   "id": "d394b1bc7151dfe2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculating variance for each column\n",
    "variances = df.var()\n",
    "\n",
    "# Identifying columns with zero variance\n",
    "zero_variance_columns = variances[variances == 0].index.tolist()\n",
    "\n",
    "print(\"Columns with zero variance:\", zero_variance_columns)\n",
    "\n",
    "# Filtering out columns with zero variance\n",
    "df = df.loc[:, variances > 0]"
   ],
   "id": "45db187f2eb50faa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Remove the columns with one unique value"
   ],
   "id": "505c63729a7552e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Checking the number of unique values per column\n",
    "unique_counts = df.nunique()\n",
    "\n",
    "# Identifying columns with only one unique value\n",
    "columns_to_remove = unique_counts[unique_counts == 1].index.tolist()\n",
    "\n",
    "# Removing these columns from the DataFrame\n",
    "df.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "# Printing the names of the columns that were removed\n",
    "print(\"Columns removed:\", columns_to_remove)"
   ],
   "id": "a9c18d542fce5019",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [],
   "id": "47f3a28c0d6281f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Correlation matrix"
   ],
   "id": "8a8dbec83754c5ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculating the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Visualizing the correlation matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.show()"
   ],
   "id": "eb87f529122d3089",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rul_correlations = correlation_matrix['RUL']\n",
    "\n",
    "# Display the correlations of 'RUL' with other features\n",
    "print(rul_correlations)"
   ],
   "id": "68cd5f79dc8e04ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.columns"
   ],
   "id": "9bc05a00504b4eaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Add lag features"
   ],
   "id": "fdc2627a9cadc23"
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate mean values across all engines for each cycle\n",
    "mean_df = df.groupby('cycle').mean().reset_index()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_mean_sensors(df, mean_df):\n",
    "    sns.set(style=\"whitegrid\")  # Set the aesthetic style of the plots\n",
    "\n",
    "    # Define columns that are not to be included in plotting (non-sensor columns)\n",
    "    exclude_columns = ['engine_id', 'cycle']\n",
    "    sensor_columns = [col for col in df.columns if col not in exclude_columns]\n",
    "\n",
    "    for sensor in sensor_columns:\n",
    "        plt.figure(figsize=(12, 6))  # Adjust the size of the figure for better visibility\n",
    "\n",
    "        # Plotting each engine's sensor data (you can remove this part if you want only the mean trendline)\n",
    "        sns.lineplot(data=df, x='cycle', y=sensor, hue='engine_id', palette='tab10', legend=None)\n",
    "        \n",
    "        # Plotting the mean trendline\n",
    "        sns.lineplot(data=mean_df, x='cycle', y=sensor, color='red', linewidth=2.5, label='Mean Trend')\n",
    "\n",
    "        plt.title(f'Sensor Readings Over Cycles for {sensor}')\n",
    "        plt.xlabel('Cycle')\n",
    "        plt.ylabel(sensor)\n",
    "        # plt.legend(title='Engine ID', bbox_to_anchor=(1.05, 1), loc='upper left')  # Removed as per requirement\n",
    "        plt.show()\n",
    "\n",
    "# Assuming your DataFrame is ready and named 'df'\n",
    "plot_mean_sensors(df, mean_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa22ca2e2b53b134",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "raw",
   "source": [
    "Some of the sensor readings show a general trend of decreasing values over time, which is expected as the engines degrade. We can use this information to create lag features that capture the rate of change in sensor readings over time. We do not use the sensor readings that show a constant value over time (or not an change in average), as they do not provide useful information for predicting RUL."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e54371f981340258"
  },
  {
   "cell_type": "code",
   "source": [
    "# define a function to create difference features that capture the rate of change in sensor readings compared to the first cycle\n",
    "\n",
    "def create_difference_features(df, sensor_columns):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original data\n",
    "    df_diff = df.copy()\n",
    "\n",
    "    # Create difference features for each sensor column\n",
    "    for sensor in sensor_columns:\n",
    "        # Create a new column name for the difference feature\n",
    "        diff_col = f'{sensor}_diff'\n",
    "\n",
    "        # Calculate the difference values\n",
    "        df_diff[diff_col] = df_diff.groupby('engine_id')[sensor].transform(lambda group: group - group.iloc[0])\n",
    "\n",
    "    return df_diff\n",
    "\n",
    "# Define the sensor columns to create difference features for\n",
    "sensor_columns = ['sensor_val2', 'sensor_val4', 'sensor_val5', 'sensor_val6',\n",
    "                  'sensor_val9', 'sensor_val10', 'sensor_val12', 'sensor_val13', 'sensor_val15', 'sensor_val17', 'sensor_val19', 'sensor_val21']\n",
    "\n",
    "# Create difference features for the selected sensor columns\n",
    "df_diff = create_difference_features(df, sensor_columns)\n",
    "\n",
    "# Only keep difference features and the 'engine_id' and 'cycle' columns\n",
    "df_diff_clean = df_diff[['engine_id', 'cycle'] + [col for col in df_diff.columns if 'diff' in col]]\n",
    "\n",
    "# Left join the difference features with the original DataFrame on 'engine_id' and 'cycle'\n",
    "df = df.merge(df_diff_clean, on=['engine_id', 'cycle'], how='left')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-22T13:33:58.554113Z"
    }
   },
   "id": "e733c87fd619af4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-22T13:33:58.691406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.head(100)"
   ],
   "id": "6187cbdc884458d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modeling linear regression"
   ],
   "id": "1ead12867c031109"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-22T13:33:58.713022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the dependent variable and independent variables\n",
    "X = df[['cycle', 'set2', 'set3', 'sensor_val1', 'sensor_val2',\n",
    "       'sensor_val3', 'sensor_val4', 'sensor_val5', 'sensor_val6',\n",
    "       'sensor_val9', 'sensor_val10', 'sensor_val12', 'sensor_val13',\n",
    "       'sensor_val15', 'sensor_val17', 'sensor_val18', 'sensor_val19',\n",
    "       'sensor_val21', 'sensor_val2_diff', 'sensor_val4_diff', 'sensor_val5_diff', 'sensor_val6_diff', 'sensor_val9_diff', 'sensor_val10_diff', 'sensor_val12_diff', 'sensor_val13_diff', 'sensor_val15_diff', 'sensor_val17_diff', 'sensor_val19_diff', 'sensor_val21_diff']]    # possible to add lag RUL\n",
    "y = df['RUL']\n",
    "\n",
    "# Adding a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Setting up the OLS model\n",
    "model = sm.OLS(y, X)\n",
    "\n",
    "# Fitting the model\n",
    "results = model.fit()\n",
    "\n",
    "# Printing the summary of the model\n",
    "print(results.summary())"
   ],
   "id": "60711a304eee2ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "eventhough set 2 had a higher correlation with RUL than set 3, controlling for other variables revealed that set 2 is not significant"
   ],
   "id": "6022e27660eccfdc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "remove set 2 as a feature, and drop other non-significant features"
   ],
   "id": "f99bd77bac4e0418"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-22T13:33:58.846126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the dependent variable and independent variables\n",
    "X = df[['cycle', 'set3', 'sensor_val1', 'sensor_val2', 'sensor_val4', 'sensor_val5', 'sensor_val6', 'sensor_val10', 'sensor_val12', 'sensor_val13',\n",
    "       'sensor_val15', 'sensor_val17', 'sensor_val1',\n",
    "       'sensor_val21', 'sensor_val2_diff', 'sensor_val5_diff', 'sensor_val6_diff', 'sensor_val9_diff', 'sensor_val13_diff', 'sensor_val15_diff', 'sensor_val17_diff', 'sensor_val19_diff', 'sensor_val21_diff']] "
   ],
   "id": "ea3b934900e3b4ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-22T13:33:58.856366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the dependent variable and independent variables\n",
    "y = df['RUL']\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Creating the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fitting the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Applying 5-fold cross-validation\n",
    "cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring=make_scorer(mean_absolute_error))\n",
    "\n",
    "# Output the results of cross-validation\n",
    "print(\"CV Scores:\", cv_scores)\n",
    "print(\"CV Average Score:\", np.mean(cv_scores))"
   ],
   "id": "2c799e9a97c33fbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-22T13:36:18.382948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Random Forest model setup\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define parameter space for Bayesian Optimization\n",
    "search_spaces = {\n",
    "    'n_estimators': Integer(50, 200),\n",
    "    'max_depth': Integer(10, 30),\n",
    "    'min_samples_split': Integer(2, 10),\n",
    "    'min_samples_leaf': Integer(1, 4)\n",
    "}\n",
    "\n",
    "# Setting up Bayesian Search\n",
    "opt = BayesSearchCV(estimator=rf, search_spaces=search_spaces, n_iter=32, scoring='neg_mean_absolute_error', n_jobs=-1, cv=5, verbose=2, random_state=42)\n",
    "\n",
    "# Fitting Bayesian Search\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# Best model evaluation\n",
    "print(\"Best parameters:\", opt.best_params_)\n",
    "print(\"Best score (neg_mean_absolute_error):\", opt.best_score_)\n",
    "\n",
    "# Optionally, check the performance on the test set\n",
    "best_rf = opt.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "test_score = best_rf.score(X_test, y_test)\n",
    "print(\"Test set score:\", test_score)"
   ],
   "id": "eb0d95608ea004aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-22T13:36:18.416363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the file path to save the model\n",
    "model_file_path = 'best_random_forest_model.pkl'\n",
    "\n",
    "# Save the model to disk\n",
    "with open(model_file_path, 'wb') as file:\n",
    "    pickle.dump(best_rf, file)\n",
    "\n",
    "print(f\"Model saved to {model_file_path}\")"
   ],
   "id": "5171f4d066c68130",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"DataSchedule-2.csv\")\n",
    "\n",
    "# Create difference features for the selected sensor columns\n",
    "df_diff = create_difference_features(df, sensor_columns)\n",
    "\n",
    "# Only keep difference features and the 'engine_id' and 'cycle' columns\n",
    "df_diff_clean = df_diff[['engine_id', 'cycle'] + [col for col in df_diff.columns if 'diff' in col]]\n",
    "\n",
    "# Left join the difference features with the original DataFrame on 'engine_id' and 'cycle'\n",
    "df = df.merge(df_diff_clean, on=['engine_id', 'cycle'], how='left')\n",
    "\n",
    "# Specify the path to the pickled model file\n",
    "model_file_path = 'best_random_forest_model.pkl'\n",
    "# Load the model from disk\n",
    "with open(model_file_path, 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "df_features = df[['cycle', 'set3', 'sensor_val1', 'sensor_val2', 'sensor_val4', 'sensor_val5', 'sensor_val6', 'sensor_val10', 'sensor_val12', 'sensor_val13',\n",
    "       'sensor_val15', 'sensor_val17', 'sensor_val1',\n",
    "       'sensor_val21', 'sensor_val2_diff', 'sensor_val5_diff', 'sensor_val6_diff', 'sensor_val9_diff', 'sensor_val13_diff', 'sensor_val15_diff', 'sensor_val17_diff', 'sensor_val19_diff', 'sensor_val21_diff']] "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-22T13:36:18.418359Z"
    }
   },
   "id": "4f51818cc596ae75",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#Making predictions and round to the nearest integer\n",
    "predictions = np.round(model.predict(df_features))\n",
    "\n",
    "#Converting to integer type\n",
    "predictions = predictions.astype(int)\n",
    "predictions\n",
    "\n",
    "## add the predictions to df\n",
    "df['RUL'] = predictions\n",
    "df\n",
    "## Take the final row of each engine id\n",
    "# Group by 'engine_id' and get the last 'RUL' value for each engine\n",
    "last_rul_per_engine = df.groupby('engine_id')['RUL'].last()\n",
    "last_rul_per_engine_df = last_rul_per_engine.reset_index()\n",
    "# Combine 'RUL' and 'engine_id' into a single column formatted as \"RUL;id\"\n",
    "last_rul_per_engine_df['RUL;id'] = last_rul_per_engine_df['RUL'].astype(str) + ';' + last_rul_per_engine_df[\n",
    "    'engine_id'].astype(str)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-22T13:36:18.419522Z"
    }
   },
   "id": "fff37a0732f77920",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a new DataFrame with only the combined column\n",
    "result_df = last_rul_per_engine_df[['RUL;id']]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('rul_id_pairs.csv', index=False, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-22T13:36:18.421532Z"
    }
   },
   "id": "e5f3d76fafa80e1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Compare to the consultancy predictions\n",
    "df_our_predictions = pd.read_csv('rul_id_pairs.csv')\n",
    "df_our_predictions[\"RULours;engine_id\"] = df_our_predictions[\"RUL;id\"]\n",
    "df_our_predictions.drop(['RUL;id'], axis=1, inplace=True)\n",
    "df_consultancy = pd.read_csv(\"RUL_consultancy_predictions_A3-2.csv\")\n",
    "df_consultancy[\"RULconsul;engine_id\"] = df_consultancy[\"RUL;id\"]\n",
    "df_consultancy.drop(['RUL;id'], axis=1, inplace=True)\n",
    "df_combined = pd.concat([df_our_predictions, df_consultancy], axis=1)\n",
    "df_combined\n",
    "# Split the 'RUL;id' column into two new columns\n",
    "df_combined[['RULours', 'engine_id']] = df_combined['RULours;engine_id'].str.split(';', expand=True)\n",
    "df_combined[['RULconsul', 'old_id']] = df_combined['RULconsul;engine_id'].str.split(';', expand=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-22T13:36:18.421532Z"
    }
   },
   "id": "795ff438d837ac7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Drop the original columns\n",
    "df_combined.drop(['RULours;engine_id', 'RULconsul;engine_id'], axis=1, inplace=True)\n",
    "\n",
    "# drop the old_id\n",
    "df_combined.drop(['old_id'], axis=1, inplace=True)\n",
    "\n",
    "#Convert these columns to an appropriate type (e.g., int)\n",
    "df_combined['RULours'] = df_combined['RULours'].astype(float)\n",
    "df_combined['RULconsul'] = df_combined['RULconsul'].astype(float)\n",
    "df_combined['engine_id'] = df_combined['engine_id'].astype(int)\n",
    "df_combined\n",
    "# Plotting histograms\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size for better readability\n",
    "\n",
    "# Histogram for RULours\n",
    "plt.hist(df_combined['RULours'], bins=10, alpha=0.5, label='RULours', color='blue')\n",
    "\n",
    "# Histogram for RULconsul\n",
    "plt.hist(df_combined['RULconsul'], bins=10, alpha=0.5, label='RULconsul', color='red')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Remaining Useful Life')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of RULours and RULconsul')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "# Calculate the difference between RULours and RULconsul\n",
    "df_combined['Difference'] = df_combined['RULours'] - df_combined['RULconsul']\n",
    "\n",
    "# Plotting the histogram of the differences\n",
    "plt.figure(figsize=(8, 6))  # Set the figure size for better readability\n",
    "plt.hist(df_combined['Difference'], bins=10, color='green', alpha=0.7)\n",
    "plt.xlabel('Difference in RUL (RULours - RULconsul)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Difference between RULours and RULconsul')\n",
    "plt.grid(True)  # Optional: adds a grid for easier readability\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-22T13:36:18.423532Z"
    }
   },
   "id": "14051a8d47af2ea9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def bootstrap_diff(data, n_bootstrap=1000):\n",
    "    bootstrap_means = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Sample with replacement from the data\n",
    "        sample = data.sample(n=len(data), replace=True)\n",
    "        # Calculate the mean difference\n",
    "        mean_diff = sample['RULours'].mean() - sample['RULconsul'].mean()\n",
    "        bootstrap_means.append(mean_diff)\n",
    "    return bootstrap_means\n",
    "\n",
    "\n",
    "# Performing bootstrap sampling\n",
    "diffs = bootstrap_diff(df_combined)\n",
    "\n",
    "# Plotting the distribution of bootstrap differences\n",
    "plt.hist(diffs, bins=30, color='blue', alpha=0.7)\n",
    "plt.xlabel('Mean Difference (RULours - RULconsul)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bootstrap Distribution of Mean Differences')\n",
    "plt.axvline(x=0, color='red', linestyle='--')\n",
    "plt.show()\n",
    "# Calculating the 95% confidence interval\n",
    "conf_interval = np.percentile(diffs, [2.5, 97.5])\n",
    "print(\"95% Confidence Interval for the Mean Difference:\", conf_interval)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-22T13:36:18.424531Z"
    }
   },
   "id": "9f50e4a0064ec720",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T13:36:18.462108Z",
     "start_time": "2024-05-22T13:36:18.425583Z"
    }
   },
   "id": "52530586fd83d52e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
